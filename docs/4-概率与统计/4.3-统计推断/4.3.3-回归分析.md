# 回归分析

## 概述

回归分析是统计推断的重要分支，用于研究变量之间的依赖关系。本模块涵盖线性回归、多元回归、非线性回归等核心方法，为数据分析和预测建模提供理论基础。

## 历史与发展

### 历史背景

- **早期发展**：19世纪高尔顿研究遗传学中的回归现象
- **费希尔时代**：20世纪初建立现代回归分析理论
- **现代发展**：广义线性模型、非参数回归、机器学习
- **应用扩展**：经济学、医学、工程学、数据科学等

### 数学意义

```lean
-- 回归分析的形式化定义
def regression_analysis (Y X : Ω → ℝ) (β : ℝᵏ) :=
  Y = f(X, β) + ε ∧ E ε = 0 ∧ V ε = σ²

-- 线性回归模型
def linear_regression_model (Y X : Ω → ℝ) (β₀ β₁ : ℝ) :=
  Y = β₀ + β₁X + ε ∧ E ε = 0 ∧ V ε = σ²
```

## 线性回归

### 模型定义

#### 基本模型

```lean
-- 线性回归模型
def linear_regression_model (Y X : Ω → ℝ) (β₀ β₁ : ℝ) :=
  Y = β₀ + β₁X + ε ∧ E ε = 0 ∧ V ε = σ²

-- 最小二乘估计
def least_squares_estimation (Y X : Ω → ℝ) :=
  (β̂₀, β̂₁) = argmin_{β₀,β₁} ∑ᵢ (Yᵢ - β₀ - β₁Xᵢ)²
```

#### 模型假设

```lean
-- 线性回归假设
def linear_regression_assumptions (Y X : Ω → ℝ) :=
  (linearity : Y = β₀ + β₁X + ε) ∧
  (independence : εᵢ ⊥ εⱼ for i ≠ j) ∧
  (homoscedasticity : V εᵢ = σ²) ∧
  (normality : ε ~ N(0, σ²))
```

### 参数估计

#### 最小二乘估计

```lean
-- 回归系数估计
theorem regression_coefficients (Y X : Ω → ℝ) :
  β̂₁ = S_{XY} / S_{XX} ∧
  β̂₀ = Ȳ - β̂₁X̄ :=
begin
  -- 证明过程
end

-- 估计量性质
theorem estimator_properties (Y X : Ω → ℝ) :
  E β̂₁ = β₁ ∧
  V β̂₁ = σ² / S_{XX} :=
begin
  -- 证明过程
end
```

#### 算法实现

```rust
// 线性回归
struct LinearRegression;

impl LinearRegression {
    fn fit(&self, x: &[f64], y: &[f64]) -> (f64, f64) {
        let n = x.len() as f64;
        let x_mean = x.iter().sum::<f64>() / n;
        let y_mean = y.iter().sum::<f64>() / n;
        
        let numerator: f64 = x.iter().zip(y.iter())
            .map(|(xi, yi)| (xi - x_mean) * (yi - y_mean))
            .sum();
        
        let denominator: f64 = x.iter()
            .map(|xi| (xi - x_mean).powi(2))
            .sum();
        
        let beta1 = numerator / denominator;
        let beta0 = y_mean - beta1 * x_mean;
        
        (beta0, beta1)
    }
    
    fn predict(&self, x: f64, beta0: f64, beta1: f64) -> f64 {
        beta0 + beta1 * x
    }
    
    fn r_squared(&self, x: &[f64], y: &[f64], beta0: f64, beta1: f64) -> f64 {
        let y_mean = y.iter().sum::<f64>() / y.len() as f64;
        
        let ss_res: f64 = x.iter().zip(y.iter())
            .map(|(xi, yi)| {
                let y_pred = self.predict(*xi, beta0, beta1);
                (yi - y_pred).powi(2)
            })
            .sum();
        
        let ss_tot: f64 = y.iter()
            .map(|yi| (yi - y_mean).powi(2))
            .sum();
        
        1.0 - ss_res / ss_tot
    }
    
    fn confidence_intervals(&self, x: &[f64], y: &[f64], confidence_level: f64) -> ((f64, f64), (f64, f64)) {
        let (beta0, beta1) = self.fit(x, y);
        let n = x.len() as f64;
        
        // 计算残差
        let residuals: Vec<f64> = x.iter().zip(y.iter())
            .map(|(xi, yi)| yi - (beta0 + beta1 * xi))
            .collect();
        
        let mse = residuals.iter()
            .map(|r| r.powi(2))
            .sum::<f64>() / (n - 2.0);
        
        let x_mean = x.iter().sum::<f64>() / n;
        let sxx = x.iter()
            .map(|xi| (xi - x_mean).powi(2))
            .sum::<f64>();
        
        let t_score = match confidence_level {
            0.95 => 2.0,
            0.99 => 2.6,
            _ => 2.0,
        };
        
        let se_beta0 = (mse * (1.0/n + x_mean.powi(2)/sxx)).sqrt();
        let se_beta1 = (mse / sxx).sqrt();
        
        let beta0_ci = (beta0 - t_score * se_beta0, beta0 + t_score * se_beta0);
        let beta1_ci = (beta1 - t_score * se_beta1, beta1 + t_score * se_beta1);
        
        (beta0_ci, beta1_ci)
    }
    
    fn prediction_interval(&self, x: &[f64], y: &[f64], x_new: f64, confidence_level: f64) -> (f64, f64) {
        let (beta0, beta1) = self.fit(x, y);
        let n = x.len() as f64;
        
        let residuals: Vec<f64> = x.iter().zip(y.iter())
            .map(|(xi, yi)| yi - (beta0 + beta1 * xi))
            .collect();
        
        let mse = residuals.iter()
            .map(|r| r.powi(2))
            .sum::<f64>() / (n - 2.0);
        
        let x_mean = x.iter().sum::<f64>() / n;
        let sxx = x.iter()
            .map(|xi| (xi - x_mean).powi(2))
            .sum::<f64>();
        
        let t_score = match confidence_level {
            0.95 => 2.0,
            0.99 => 2.6,
            _ => 2.0,
        };
        
        let y_pred = self.predict(x_new, beta0, beta1);
        let se_pred = (mse * (1.0 + 1.0/n + (x_new - x_mean).powi(2)/sxx)).sqrt();
        let margin = t_score * se_pred;
        
        (y_pred - margin, y_pred + margin)
    }
}
```

### 模型诊断

#### 残差分析

```rust
// 残差分析
struct ResidualAnalysis;

impl ResidualAnalysis {
    fn calculate_residuals(&self, x: &[f64], y: &[f64], beta0: f64, beta1: f64) -> Vec<f64> {
        x.iter().zip(y.iter())
            .map(|(xi, yi)| yi - (beta0 + beta1 * xi))
            .collect()
    }
    
    fn residual_plot(&self, x: &[f64], residuals: &[f64]) -> Vec<(f64, f64)> {
        x.iter().zip(residuals.iter())
            .map(|(xi, ri)| (*xi, *ri))
            .collect()
    }
    
    fn normal_probability_plot(&self, residuals: &[f64]) -> Vec<(f64, f64)> {
        let mut sorted_residuals = residuals.to_vec();
        sorted_residuals.sort_by(|a, b| a.partial_cmp(b).unwrap());
        
        let n = residuals.len() as f64;
        sorted_residuals.iter().enumerate()
            .map(|(i, &residual)| {
                let p = (i as f64 + 0.5) / n;
                let z_score = self.normal_quantile(p);
                (z_score, residual)
            })
            .collect()
    }
    
    fn normal_quantile(&self, p: f64) -> f64 {
        // 简化版本，实际应用中需要使用正态分布的反函数
        if p < 0.5 {
            -1.0 * (1.0 - 2.0 * p).sqrt()
        } else {
            (2.0 * p - 1.0).sqrt()
        }
    }
    
    fn durbin_watson_test(&self, residuals: &[f64]) -> f64 {
        let n = residuals.len();
        if n < 2 {
            return 0.0;
        }
        
        let numerator: f64 = (1..n)
            .map(|i| (residuals[i] - residuals[i-1]).powi(2))
            .sum();
        
        let denominator: f64 = residuals.iter()
            .map(|r| r.powi(2))
            .sum();
        
        numerator / denominator
    }
}
```

#### 影响分析

```rust
// 影响分析
struct InfluenceAnalysis;

impl InfluenceAnalysis {
    fn leverage(&self, x: &[f64], i: usize) -> f64 {
        let n = x.len() as f64;
        let x_mean = x.iter().sum::<f64>() / n;
        let sxx = x.iter()
            .map(|xi| (xi - x_mean).powi(2))
            .sum::<f64>();
        
        let xi = x[i];
        (1.0/n + (xi - x_mean).powi(2) / sxx)
    }
    
    fn cook_distance(&self, x: &[f64], y: &[f64], i: usize) -> f64 {
        let (beta0, beta1) = LinearRegression.fit(x, y);
        let y_pred_i = LinearRegression.predict(x[i], beta0, beta1);
        
        // 删除第i个观测值重新拟合
        let x_without_i: Vec<f64> = x.iter().enumerate()
            .filter(|(j, _)| *j != i)
            .map(|(_, &val)| val)
            .collect();
        
        let y_without_i: Vec<f64> = y.iter().enumerate()
            .filter(|(j, _)| *j != i)
            .map(|(_, &val)| val)
            .collect();
        
        let (beta0_new, beta1_new) = LinearRegression.fit(&x_without_i, &y_without_i);
        let y_pred_i_new = LinearRegression.predict(x[i], beta0_new, beta1_new);
        
        let mse = self.calculate_mse(x, y, beta0, beta1);
        let leverage = self.leverage(x, i);
        
        ((y_pred_i - y_pred_i_new).powi(2) * leverage) / (2.0 * mse)
    }
    
    fn calculate_mse(&self, x: &[f64], y: &[f64], beta0: f64, beta1: f64) -> f64 {
        let residuals: Vec<f64> = x.iter().zip(y.iter())
            .map(|(xi, yi)| yi - (beta0 + beta1 * xi))
            .collect();
        
        residuals.iter()
            .map(|r| r.powi(2))
            .sum::<f64>() / (x.len() - 2) as f64
    }
}
```

## 多元回归

### 模型定义21

#### 基本模型21

```lean
-- 多元线性回归模型
def multiple_linear_regression (Y : Ω → ℝ) (X : Ω → ℝᵏ) (β : ℝᵏ⁺¹) :=
  Y = β₀ + ∑ᵢ βᵢXᵢ + ε ∧ E ε = 0 ∧ V ε = σ²

-- 矩阵形式
def matrix_form (Y : ℝⁿ) (X : ℝⁿˣᵏ) (β : ℝᵏ) :=
  Y = Xβ + ε
```

#### 参数估计22

```lean
-- 最小二乘估计（矩阵形式）
theorem ols_estimation (Y : ℝⁿ) (X : ℝⁿˣᵏ) :
  β̂ = (X'X)⁻¹X'Y :=
begin
  -- 证明过程
end

-- 估计量性质
theorem ols_properties (Y : ℝⁿ) (X : ℝⁿˣᵏ) :
  E β̂ = β ∧
  V β̂ = σ²(X'X)⁻¹ :=
begin
  -- 证明过程
end
```

#### 算法实现23

```rust
// 多元线性回归
struct MultipleLinearRegression;

impl MultipleLinearRegression {
    fn fit(&self, x: &[Vec<f64>], y: &[f64]) -> Vec<f64> {
        let n = x.len();
        let p = x[0].len();
        
        // 构建设计矩阵
        let mut design_matrix = vec![vec![1.0; n]; p + 1];
        for i in 0..n {
            for j in 0..p {
                design_matrix[j + 1][i] = x[i][j];
            }
        }
        
        // 计算 X'X 和 X'y
        let mut xtx = vec![vec![0.0; p + 1]; p + 1];
        let mut xty = vec![0.0; p + 1];
        
        for i in 0..(p + 1) {
            for j in 0..(p + 1) {
                for k in 0..n {
                    xtx[i][j] += design_matrix[i][k] * design_matrix[j][k];
                }
            }
            
            for k in 0..n {
                xty[i] += design_matrix[i][k] * y[k];
            }
        }
        
        // 求解线性方程组 (X'X)β = X'y
        self.solve_linear_system(&xtx, &xty)
    }
    
    fn solve_linear_system(&self, a: &[Vec<f64>], b: &[f64]) -> Vec<f64> {
        let n = a.len();
        let mut augmented = vec![vec![0.0; n + 1]; n];
        
        for i in 0..n {
            for j in 0..n {
                augmented[i][j] = a[i][j];
            }
            augmented[i][n] = b[i];
        }
        
        // 高斯消元法
        for i in 0..n {
            let mut max_row = i;
            for k in (i + 1)..n {
                if augmented[k][i].abs() > augmented[max_row][i].abs() {
                    max_row = k;
                }
            }
            
            if max_row != i {
                augmented.swap(i, max_row);
            }
            
            for k in (i + 1)..n {
                let factor = augmented[k][i] / augmented[i][i];
                for j in i..(n + 1) {
                    augmented[k][j] -= factor * augmented[i][j];
                }
            }
        }
        
        // 回代求解
        let mut x = vec![0.0; n];
        for i in (0..n).rev() {
            let mut sum = 0.0;
            for j in (i + 1)..n {
                sum += augmented[i][j] * x[j];
            }
            x[i] = (augmented[i][n] - sum) / augmented[i][i];
        }
        
        x
    }
    
    fn predict(&self, x: &[f64], coefficients: &[f64]) -> f64 {
        let mut prediction = coefficients[0]; // 截距项
        for (i, xi) in x.iter().enumerate() {
            prediction += coefficients[i + 1] * xi;
        }
        prediction
    }
    
    fn r_squared(&self, x: &[Vec<f64>], y: &[f64], coefficients: &[f64]) -> f64 {
        let y_mean = y.iter().sum::<f64>() / y.len() as f64;
        
        let ss_res: f64 = x.iter().zip(y.iter())
            .map(|(xi, yi)| {
                let y_pred = self.predict(xi, coefficients);
                (yi - y_pred).powi(2)
            })
            .sum();
        
        let ss_tot: f64 = y.iter()
            .map(|yi| (yi - y_mean).powi(2))
            .sum();
        
        1.0 - ss_res / ss_tot
    }
    
    fn adjusted_r_squared(&self, x: &[Vec<f64>], y: &[f64], coefficients: &[f64]) -> f64 {
        let r_squared = self.r_squared(x, y, coefficients);
        let n = x.len() as f64;
        let p = x[0].len() as f64;
        
        1.0 - (1.0 - r_squared) * (n - 1.0) / (n - p - 1.0)
    }
}
```

### 模型选择

#### 逐步回归

```rust
// 逐步回归
struct StepwiseRegression;

impl StepwiseRegression {
    fn forward_selection(&self, x: &[Vec<f64>], y: &[f64], alpha: f64) -> Vec<usize> {
        let mut selected_variables = Vec::new();
        let mut remaining_variables: Vec<usize> = (0..x[0].len()).collect();
        
        while !remaining_variables.is_empty() {
            let mut best_variable = None;
            let mut best_f_stat = 0.0;
            
            for &var in &remaining_variables {
                let mut current_vars = selected_variables.clone();
                current_vars.push(var);
                
                let f_stat = self.calculate_f_statistic(x, y, &current_vars);
                if f_stat > best_f_stat {
                    best_f_stat = f_stat;
                    best_variable = Some(var);
                }
            }
            
            if let Some(var) = best_variable {
                let p_value = self.f_statistic_p_value(best_f_stat, selected_variables.len() + 1, x.len());
                if p_value < alpha {
                    selected_variables.push(var);
                    remaining_variables.retain(|&v| v != var);
                } else {
                    break;
                }
            } else {
                break;
            }
        }
        
        selected_variables
    }
    
    fn backward_elimination(&self, x: &[Vec<f64>], y: &[f64], alpha: f64) -> Vec<usize> {
        let mut selected_variables: Vec<usize> = (0..x[0].len()).collect();
        
        while selected_variables.len() > 1 {
            let mut worst_variable = None;
            let mut worst_f_stat = f64::INFINITY;
            
            for &var in &selected_variables {
                let mut current_vars = selected_variables.clone();
                current_vars.retain(|&v| v != var);
                
                let f_stat = self.calculate_f_statistic(x, y, &current_vars);
                if f_stat < worst_f_stat {
                    worst_f_stat = f_stat;
                    worst_variable = Some(var);
                }
            }
            
            if let Some(var) = worst_variable {
                let p_value = self.f_statistic_p_value(worst_f_stat, selected_variables.len() - 1, x.len());
                if p_value > alpha {
                    selected_variables.retain(|&v| v != var);
                } else {
                    break;
                }
            } else {
                break;
            }
        }
        
        selected_variables
    }
    
    fn calculate_f_statistic(&self, x: &[Vec<f64>], y: &[f64], variables: &[usize]) -> f64 {
        // 简化版本，实际应用中需要更复杂的计算
        let regression = MultipleLinearRegression;
        let selected_x: Vec<Vec<f64>> = x.iter()
            .map(|row| variables.iter().map(|&i| row[i]).collect())
            .collect();
        
        let coefficients = regression.fit(&selected_x, y);
        let r_squared = regression.r_squared(&selected_x, y, &coefficients);
        
        let n = x.len() as f64;
        let p = variables.len() as f64;
        
        (r_squared / p) / ((1.0 - r_squared) / (n - p - 1.0))
    }
    
    fn f_statistic_p_value(&self, f_stat: f64, df1: usize, df2: usize) -> f64 {
        // 简化版本，实际应用中需要使用F分布
        0.05
    }
}
```

## 非线性回归

### 多项式回归

```rust
// 多项式回归
struct PolynomialRegression;

impl PolynomialRegression {
    fn fit(&self, x: &[f64], y: &[f64], degree: usize) -> Vec<f64> {
        let n = x.len();
        let mut design_matrix = vec![vec![0.0; degree + 1]; n];
        
        for i in 0..n {
            for j in 0..(degree + 1) {
                design_matrix[i][j] = x[i].powi(j as i32);
            }
        }
        
        let regression = MultipleLinearRegression;
        regression.fit(&design_matrix, y)
    }
    
    fn predict(&self, x: f64, coefficients: &[f64]) -> f64 {
        coefficients.iter().enumerate()
            .map(|(i, &coef)| coef * x.powi(i as i32))
            .sum()
    }
    
    fn cross_validation(&self, x: &[f64], y: &[f64], max_degree: usize, folds: usize) -> usize {
        let mut best_degree = 1;
        let mut best_mse = f64::INFINITY;
        
        for degree in 1..=max_degree {
            let mse = self.k_fold_cv(x, y, degree, folds);
            if mse < best_mse {
                best_mse = mse;
                best_degree = degree;
            }
        }
        
        best_degree
    }
    
    fn k_fold_cv(&self, x: &[f64], y: &[f64], degree: usize, folds: usize) -> f64 {
        let n = x.len();
        let fold_size = n / folds;
        let mut total_mse = 0.0;
        
        for fold in 0..folds {
            let start = fold * fold_size;
            let end = if fold == folds - 1 { n } else { (fold + 1) * fold_size };
            
            let mut train_x = Vec::new();
            let mut train_y = Vec::new();
            let mut test_x = Vec::new();
            let mut test_y = Vec::new();
            
            for i in 0..n {
                if i >= start && i < end {
                    test_x.push(x[i]);
                    test_y.push(y[i]);
                } else {
                    train_x.push(x[i]);
                    train_y.push(y[i]);
                }
            }
            
            let coefficients = self.fit(&train_x, &train_y, degree);
            let mse = self.calculate_mse(&test_x, &test_y, &coefficients);
            total_mse += mse;
        }
        
        total_mse / folds as f64
    }
    
    fn calculate_mse(&self, x: &[f64], y: &[f64], coefficients: &[f64]) -> f64 {
        let n = x.len() as f64;
        x.iter().zip(y.iter())
            .map(|(xi, yi)| {
                let y_pred = self.predict(*xi, coefficients);
                (yi - y_pred).powi(2)
            })
            .sum::<f64>() / n
    }
}
```

### 广义线性模型

```rust
// 广义线性模型
struct GeneralizedLinearModel;

impl GeneralizedLinearModel {
    fn logistic_regression(&self, x: &[Vec<f64>], y: &[bool]) -> Vec<f64> {
        let n = x.len();
        let p = x[0].len();
        let mut beta = vec![0.0; p + 1];
        
        // 使用牛顿-拉夫森方法
        for _ in 0..100 {
            let mut gradient = vec![0.0; p + 1];
            let mut hessian = vec![vec![0.0; p + 1]; p + 1];
            
            for i in 0..n {
                let mut x_i = vec![1.0];
                x_i.extend_from_slice(&x[i]);
                
                let eta: f64 = x_i.iter().zip(beta.iter()).map(|(xi, bi)| xi * bi).sum();
                let mu = 1.0 / (1.0 + (-eta).exp());
                
                let residual = if y[i] { 1.0 } else { 0.0 } - mu;
                let weight = mu * (1.0 - mu);
                
                for j in 0..(p + 1) {
                    gradient[j] += residual * x_i[j];
                    for k in 0..(p + 1) {
                        hessian[j][k] -= weight * x_i[j] * x_i[k];
                    }
                }
            }
            
            let delta = self.solve_linear_system(&hessian, &gradient);
            for i in 0..(p + 1) {
                beta[i] -= delta[i];
            }
        }
        
        beta
    }
    
    fn solve_linear_system(&self, a: &[Vec<f64>], b: &[f64]) -> Vec<f64> {
        // 简化版本，实际应用中需要更稳定的算法
        let n = a.len();
        let mut x = vec![0.0; n];
        
        for i in 0..n {
            x[i] = b[i] / a[i][i];
        }
        
        x
    }
    
    fn predict_probability(&self, x: &[f64], coefficients: &[f64]) -> f64 {
        let mut eta = coefficients[0];
        for (i, xi) in x.iter().enumerate() {
            eta += coefficients[i + 1] * xi;
        }
        
        1.0 / (1.0 + (-eta).exp())
    }
}
```

## 实际应用

### 经典问题

#### 房价预测

```rust
// 房价预测模型
struct HousePriceModel;

impl HousePriceModel {
    fn simple_linear_model(&self, square_feet: &[f64], prices: &[f64]) -> (f64, f64) {
        let regression = LinearRegression;
        regression.fit(square_feet, prices)
    }
    
    fn multiple_regression_model(&self, features: &[Vec<f64>], prices: &[f64]) -> Vec<f64> {
        let regression = MultipleLinearRegression;
        regression.fit(features, prices)
    }
    
    fn polynomial_model(&self, square_feet: &[f64], prices: &[f64]) -> Vec<f64> {
        let regression = PolynomialRegression;
        regression.fit(square_feet, prices, 2)
    }
    
    fn cross_validate(&self, features: &[Vec<f64>], prices: &[f64]) -> f64 {
        let regression = MultipleLinearRegression;
        let coefficients = regression.fit(features, prices);
        
        let predictions: Vec<f64> = features.iter()
            .map(|x| regression.predict(x, &coefficients))
            .collect();
        
        let mse = predictions.iter().zip(prices.iter())
            .map(|(pred, actual)| (pred - actual).powi(2))
            .sum::<f64>() / prices.len() as f64;
        
        mse
    }
}
```

#### 销售预测

```rust
// 销售预测模型
struct SalesPredictionModel;

impl SalesPredictionModel {
    fn time_series_regression(&self, time: &[f64], sales: &[f64]) -> (f64, f64) {
        let regression = LinearRegression;
        regression.fit(time, sales)
    }
    
    fn seasonal_model(&self, time: &[f64], sales: &[f64], season_length: usize) -> Vec<f64> {
        let n = time.len();
        let mut features = vec![vec![0.0; 2 * season_length + 1]; n];
        
        for i in 0..n {
            features[i][0] = time[i]; // 趋势项
            
            for j in 0..season_length {
                features[i][j + 1] = if i % season_length == j { 1.0 } else { 0.0 }; // 季节性虚拟变量
                features[i][j + season_length + 1] = time[i] * features[i][j + 1]; // 趋势-季节性交互
            }
        }
        
        let regression = MultipleLinearRegression;
        regression.fit(&features, sales)
    }
    
    fn forecast(&self, time: f64, coefficients: &[f64], season_length: usize) -> f64 {
        let mut prediction = coefficients[0] * time; // 趋势项
        
        for j in 0..season_length {
            let season_index = (time as usize) % season_length;
            if season_index == j {
                prediction += coefficients[j + 1]; // 季节性效应
                prediction += coefficients[j + season_length + 1] * time; // 趋势-季节性交互
            }
        }
        
        prediction
    }
}
```

### 复杂问题

#### 特征选择

```rust
// 特征选择
struct FeatureSelection;

impl FeatureSelection {
    fn lasso_regression(&self, x: &[Vec<f64>], y: &[f64], lambda: f64) -> Vec<f64> {
        let n = x.len();
        let p = x[0].len();
        let mut beta = vec![0.0; p];
        
        // 坐标下降法
        for iteration in 0..1000 {
            let mut max_change = 0.0;
            
            for j in 0..p {
                let mut numerator = 0.0;
                let mut denominator = 0.0;
                
                for i in 0..n {
                    let mut residual = y[i];
                    for k in 0..p {
                        if k != j {
                            residual -= beta[k] * x[i][k];
                        }
                    }
                    
                    numerator += residual * x[i][j];
                    denominator += x[i][j].powi(2);
                }
                
                let old_beta = beta[j];
                let soft_threshold = lambda / (2.0 * denominator);
                
                if numerator > soft_threshold {
                    beta[j] = (numerator - soft_threshold) / denominator;
                } else if numerator < -soft_threshold {
                    beta[j] = (numerator + soft_threshold) / denominator;
                } else {
                    beta[j] = 0.0;
                }
                
                max_change = max_change.max((beta[j] - old_beta).abs());
            }
            
            if max_change < 1e-6 {
                break;
            }
        }
        
        beta
    }
    
    fn ridge_regression(&self, x: &[Vec<f64>], y: &[f64], lambda: f64) -> Vec<f64> {
        let n = x.len();
        let p = x[0].len();
        
        // 构建增广矩阵
        let mut augmented_x = vec![vec![0.0; p]; n + p];
        let mut augmented_y = vec![0.0; n + p];
        
        for i in 0..n {
            for j in 0..p {
                augmented_x[i][j] = x[i][j];
            }
            augmented_y[i] = y[i];
        }
        
        for i in 0..p {
            augmented_x[n + i][i] = lambda.sqrt();
            augmented_y[n + i] = 0.0;
        }
        
        let regression = MultipleLinearRegression;
        regression.fit(&augmented_x, &augmented_y)
    }
    
    fn elastic_net(&self, x: &[Vec<f64>], y: &[f64], lambda: f64, alpha: f64) -> Vec<f64> {
        let lasso_lambda = lambda * alpha;
        let ridge_lambda = lambda * (1.0 - alpha);
        
        let lasso_coef = self.lasso_regression(x, y, lasso_lambda);
        let ridge_coef = self.ridge_regression(x, y, ridge_lambda);
        
        // 组合Lasso和Ridge的结果
        lasso_coef.iter().zip(ridge_coef.iter())
            .map(|(l, r)| 0.5 * (l + r))
            .collect()
    }
}
```

## 教学策略

### 多表征学习

- **符号表征**：数学公式和符号
- **图形表征**：散点图、回归线、残差图
- **数值表征**：具体数值和计算
- **物理表征**：实际数据和实验

### 认知负荷管理

- **分步教学**：从简单到复杂
- **渐进复杂**：逐步增加难度
- **多角度验证**：多种方法验证结果

### 错误预防

- **常见错误**：
  - 混淆相关性和因果关系
  - 忽略模型假设
  - 过度拟合
- **预防策略**：
  - 强调概念理解
  - 多练习和验证
  - 系统化检查

## 评估标准

### 理解层面

- 能否解释回归分析概念
- 能否理解不同回归方法
- 能否识别适用条件

### 应用层面

- 能否正确进行回归分析
- 能否诊断模型问题
- 能否解决实际问题

### 创新层面

- 能否发现新的应用
- 能否优化回归方法
- 能否建立新的联系

## 扩展阅读

### 理论扩展

- **广义线性模型**：逻辑回归、泊松回归
- **非参数回归**：核回归、样条回归
- **时间序列回归**：ARIMA模型、协整分析

### 应用扩展

- **机器学习**：监督学习、特征工程
- **数据科学**：预测建模、模型验证
- **经济学**：计量经济学、因果推断

### 历史文献

- 高斯《误差理论》
- 费希尔《统计推断》
- 奈曼《统计假设检验》

## 学习资源

### 推荐教材

- 《应用回归分析》- 韦斯伯格
- 《统计学习基础》- 哈斯蒂
- 《计量经济学》- 伍德里奇

### 在线资源

- Coursera机器学习课程
- edX数据科学课程
- MIT OpenCourseWare统计课程

### 实践工具

- R语言：lm, glm函数
- Python：scikit-learn
- SAS：PROC REG

---

*回归分析为数据分析和预测建模提供了强大的数学工具，是现代统计学和机器学习的重要基础。*
