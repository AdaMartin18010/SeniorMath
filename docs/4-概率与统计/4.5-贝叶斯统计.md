# 贝叶斯统计

## 概述

贝叶斯统计是统计推断的重要分支，基于贝叶斯定理将先验知识与观测数据相结合。本模块涵盖贝叶斯推断、先验分布、后验分布、贝叶斯决策理论等核心内容，为不确定性量化提供理论基础。

## 历史与发展

### 历史背景

- **早期发展**：18世纪贝叶斯提出贝叶斯定理
- **理论完善**：拉普拉斯、杰弗里斯建立现代贝叶斯理论
- **现代发展**：马尔可夫链蒙特卡洛、变分推断
- **应用扩展**：机器学习、人工智能、决策科学等

### 数学意义

```lean
-- 贝叶斯定理的形式化定义
def bayes_theorem (A B : set Ω) (h : P B > 0) :=
  P (B | A) = P (A | B) * P B / P A

-- 贝叶斯推断
def bayesian_inference (θ : parameter) (X : data) :=
  P (θ | X) ∝ P (X | θ) * P (θ)
```

## 贝叶斯推断基础

### 贝叶斯定理

#### 基本定理

```lean
-- 贝叶斯定理
theorem bayes_theorem (A B : set Ω) (h : P B > 0) :
  P (B | A) = P (A | B) * P B / P A :=
begin
  -- 证明过程
end

-- 全概率公式
theorem law_of_total_probability (A : set Ω) (Bᵢ : set Ω) :
  P A = ∑ᵢ P (A | Bᵢ) * P Bᵢ :=
begin
  -- 证明过程
end
```

#### 算法实现

```rust
// 贝叶斯推断
struct BayesianInference;

impl BayesianInference {
    fn bayes_theorem(&self, p_a_given_b: f64, p_b: f64, p_a: f64) -> f64 {
        if p_a > 0.0 {
            p_a_given_b * p_b / p_a
        } else {
            0.0
        }
    }
    
    fn law_of_total_probability(&self, conditional_probs: &[f64], prior_probs: &[f64]) -> f64 {
        conditional_probs.iter()
            .zip(prior_probs.iter())
            .map(|(cond, prior)| cond * prior)
            .sum()
    }
    
    fn posterior_probability(&self, likelihood: f64, prior: f64, evidence: f64) -> f64 {
        if evidence > 0.0 {
            likelihood * prior / evidence
        } else {
            0.0
        }
    }
    
    fn bayes_factor(&self, likelihood_h1: f64, likelihood_h0: f64) -> f64 {
        likelihood_h1 / likelihood_h0
    }
}
```

### 先验分布

#### 共轭先验

```lean
-- 共轭先验定义
def conjugate_prior (likelihood : distribution) (prior : distribution) :=
  posterior ∝ likelihood × prior ∧
  posterior ∈ same_family_as prior

-- 常见共轭先验
def beta_binomial_conjugate :=
  prior : Beta(α, β) ∧
  likelihood : Binomial(n, θ) ∧
  posterior : Beta(α + x, β + n - x)
```

#### 算法实现6

```rust
// 共轭先验
struct ConjugatePriors;

impl ConjugatePriors {
    fn beta_binomial_update(&self, alpha: f64, beta: f64, successes: usize, trials: usize) -> (f64, f64) {
        let new_alpha = alpha + successes as f64;
        let new_beta = beta + (trials - successes) as f64;
        (new_alpha, new_beta)
    }
    
    fn gamma_poisson_update(&self, alpha: f64, beta: f64, observations: &[usize]) -> (f64, f64) {
        let sum_obs: f64 = observations.iter().sum::<usize>() as f64;
        let n = observations.len() as f64;
        
        let new_alpha = alpha + sum_obs;
        let new_beta = beta + n;
        (new_alpha, new_beta)
    }
    
    fn normal_normal_update(&self, mu_0: f64, sigma_0: f64, data: &[f64], sigma: f64) -> (f64, f64) {
        let n = data.len() as f64;
        let x_bar = data.iter().sum::<f64>() / n;
        
        let precision_0 = 1.0 / (sigma_0 * sigma_0);
        let precision_data = n / (sigma * sigma);
        
        let new_precision = precision_0 + precision_data;
        let new_sigma = 1.0 / new_precision.sqrt();
        
        let new_mu = (precision_0 * mu_0 + precision_data * x_bar) / new_precision;
        
        (new_mu, new_sigma)
    }
    
    fn inverse_gamma_normal_update(&self, alpha: f64, beta: f64, data: &[f64], mu: f64) -> (f64, f64) {
        let n = data.len() as f64;
        let sum_squared_diff: f64 = data.iter()
            .map(|x| (x - mu).powi(2))
            .sum();
        
        let new_alpha = alpha + n / 2.0;
        let new_beta = beta + sum_squared_diff / 2.0;
        
        (new_alpha, new_beta)
    }
}
```

### 后验分布

#### 后验计算

```lean
-- 后验分布
def posterior_distribution (θ : parameter) (X : data) :=
  P (θ | X) = P (X | θ) * P (θ) / P (X)

-- 后验预测分布
def posterior_predictive (X_new : data) (X : data) :=
  P (X_new | X) = ∫ P (X_new | θ) * P (θ | X) dθ
```

#### 算法实现7

```rust
// 后验分布计算
struct PosteriorDistribution;

impl PosteriorDistribution {
    fn calculate_posterior(&self, likelihood: &[f64], prior: &[f64]) -> Vec<f64> {
        let mut posterior = Vec::new();
        let mut evidence = 0.0;
        
        for (l, p) in likelihood.iter().zip(prior.iter()) {
            posterior.push(l * p);
            evidence += l * p;
        }
        
        // 归一化
        for p in &mut posterior {
            *p /= evidence;
        }
        
        posterior
    }
    
    fn posterior_predictive(&self, likelihood_new: &[f64], posterior: &[f64]) -> f64 {
        likelihood_new.iter()
            .zip(posterior.iter())
            .map(|(l, p)| l * p)
            .sum()
    }
    
    fn credible_interval(&self, posterior: &[f64], theta_values: &[f64], confidence: f64) -> (f64, f64) {
        let mut cumulative = 0.0;
        let alpha = (1.0 - confidence) / 2.0;
        
        let mut lower_idx = 0;
        let mut upper_idx = theta_values.len() - 1;
        
        // 找到下界
        for (i, p) in posterior.iter().enumerate() {
            cumulative += p;
            if cumulative >= alpha {
                lower_idx = i;
                break;
            }
        }
        
        // 找到上界
        cumulative = 0.0;
        for (i, p) in posterior.iter().enumerate().rev() {
            cumulative += p;
            if cumulative >= alpha {
                upper_idx = i;
                break;
            }
        }
        
        (theta_values[lower_idx], theta_values[upper_idx])
    }
    
    fn bayesian_hypothesis_test(&self, h0_posterior: f64, h1_posterior: f64) -> (f64, String) {
        let odds_ratio = h1_posterior / h0_posterior;
        let bayes_factor = odds_ratio / (h1_posterior / (h0_posterior + h1_posterior));
        
        let interpretation = if bayes_factor > 100.0 {
            "决定性证据支持H1"
        } else if bayes_factor > 30.0 {
            "强证据支持H1"
        } else if bayes_factor > 10.0 {
            "中等证据支持H1"
        } else if bayes_factor > 3.0 {
            "弱证据支持H1"
        } else if bayes_factor > 1.0 {
            "无证据"
        } else if bayes_factor > 0.33 {
            "弱证据支持H0"
        } else if bayes_factor > 0.1 {
            "中等证据支持H0"
        } else if bayes_factor > 0.03 {
            "强证据支持H0"
        } else {
            "决定性证据支持H0"
        }.to_string();
        
        (bayes_factor, interpretation)
    }
}
```

## 马尔可夫链蒙特卡洛

### Metropolis-Hastings算法

#### 算法原理

```lean
-- Metropolis-Hastings算法
def metropolis_hastings (θ₀ : parameter) (target : distribution) (proposal : distribution) :=
  for t = 1 to T:
    θ* ~ proposal(θ* | θₜ₋₁)
    α = min(1, target(θ*) / target(θₜ₋₁))
    θₜ = θ* with probability α, else θₜ₋₁
```

#### 算法实现8

```rust
// Metropolis-Hastings算法
struct MetropolisHastings;

impl MetropolisHastings {
    fn sample(&self, initial_theta: f64, target_distribution: fn(f64) -> f64, 
              proposal_distribution: fn(f64) -> f64, n_samples: usize) -> Vec<f64> {
        let mut samples = Vec::new();
        let mut current_theta = initial_theta;
        
        for _ in 0..n_samples {
            // 生成候选样本
            let candidate = proposal_distribution(current_theta);
            
            // 计算接受概率
            let acceptance_ratio = target_distribution(candidate) / target_distribution(current_theta);
            let alpha = acceptance_ratio.min(1.0);
            
            // 接受或拒绝
            let u = rand::random::<f64>();
            if u < alpha {
                current_theta = candidate;
            }
            
            samples.push(current_theta);
        }
        
        samples
    }
    
    fn normal_proposal(&self, current: f64, std_dev: f64) -> f64 {
        use rand::distributions::{Normal, Distribution};
        let normal = Normal::new(current, std_dev).unwrap();
        normal.sample(&mut rand::thread_rng())
    }
    
    fn target_normal(&self, x: f64, mu: f64, sigma: f64) -> f64 {
        let z = (x - mu) / sigma;
        (-0.5 * z * z).exp() / (sigma * (2.0 * std::f64::consts::PI).sqrt())
    }
    
    fn target_beta(&self, x: f64, alpha: f64, beta: f64) -> f64 {
        if x < 0.0 || x > 1.0 {
            0.0
        } else {
            x.powf(alpha - 1.0) * (1.0 - x).powf(beta - 1.0)
        }
    }
}
```

### Gibbs采样

#### 算法原理9

```lean
-- Gibbs采样
def gibbs_sampling (θ₀ : parameter_vector) (conditional_distributions : list distribution) :=
  for t = 1 to T:
    for i = 1 to d:
      θᵢₜ ~ P(θᵢ | θ₁ₜ, ..., θᵢ₋₁ₜ, θᵢ₊₁ₜ₋₁, ..., θₙₜ₋₁)
```

#### 算法实现10

```rust
// Gibbs采样
struct GibbsSampling;

impl GibbsSampling {
    fn sample(&self, initial_params: &[f64], conditional_dists: &[fn(&[f64]) -> f64], 
              n_samples: usize) -> Vec<Vec<f64>> {
        let mut samples = Vec::new();
        let mut current_params = initial_params.to_vec();
        
        for _ in 0..n_samples {
            let mut new_params = current_params.clone();
            
            // 逐个更新每个参数
            for i in 0..current_params.len() {
                new_params[i] = conditional_dists[i](&new_params);
            }
            
            samples.push(new_params.clone());
            current_params = new_params;
        }
        
        samples
    }
    
    fn normal_conditional_mean(&self, params: &[f64], index: usize, mu: f64, sigma: f64) -> f64 {
        // 简化版本，实际应用中需要更复杂的条件分布
        mu + 0.1 * (params[index] - mu)
    }
    
    fn beta_conditional(&self, params: &[f64], index: usize, alpha: f64, beta: f64) -> f64 {
        // 简化版本，实际应用中需要更复杂的条件分布
        let current = params[index];
        (current + alpha) / (current + alpha + beta)
    }
}
```

## 贝叶斯决策理论

### 决策理论

#### 基本概念

```lean
-- 损失函数
def loss_function (a : action) (θ : state) :=
  L(a, θ) : ℝ

-- 风险函数
def risk_function (δ : decision_function) (θ : state) :=
  R(δ, θ) = E[L(δ(X), θ)]

-- 贝叶斯风险
def bayes_risk (δ : decision_function) (π : prior) :=
  R(δ, π) = ∫ R(δ, θ) π(θ) dθ
```

#### 算法实现11

```rust
// 贝叶斯决策理论
struct BayesianDecisionTheory;

impl BayesianDecisionTheory {
    fn bayes_risk(&self, decision_function: fn(&[f64]) -> f64, 
                  loss_function: fn(f64, f64) -> f64,
                  prior: &[f64], theta_values: &[f64]) -> f64 {
        let mut total_risk = 0.0;
        
        for (theta, prior_prob) in theta_values.iter().zip(prior.iter()) {
            let mut expected_loss = 0.0;
            
            // 计算给定theta下的期望损失
            for x in 0..100 {
                let x_val = x as f64 / 100.0;
                let data = vec![x_val];
                let action = decision_function(&data);
                let loss = loss_function(action, *theta);
                expected_loss += loss * 0.01; // 简化积分
            }
            
            total_risk += expected_loss * prior_prob;
        }
        
        total_risk
    }
    
    fn squared_error_loss(&self, action: f64, true_value: f64) -> f64 {
        (action - true_value).powi(2)
    }
    
    fn absolute_error_loss(&self, action: f64, true_value: f64) -> f64 {
        (action - true_value).abs()
    }
    
    fn zero_one_loss(&self, action: f64, true_value: f64) -> f64 {
        if (action - true_value).abs() < 0.1 {
            0.0
        } else {
            1.0
        }
    }
    
    fn optimal_decision(&self, posterior: &[f64], theta_values: &[f64], 
                       loss_function: fn(f64, f64) -> f64) -> f64 {
        let mut min_expected_loss = f64::INFINITY;
        let mut optimal_action = 0.0;
        
        for action in 0..100 {
            let action_val = action as f64 / 100.0;
            let mut expected_loss = 0.0;
            
            for (theta, post_prob) in theta_values.iter().zip(posterior.iter()) {
                expected_loss += loss_function(action_val, *theta) * post_prob;
            }
            
            if expected_loss < min_expected_loss {
                min_expected_loss = expected_loss;
                optimal_action = action_val;
            }
        }
        
        optimal_action
    }
}
```

### 贝叶斯网络

#### 网络结构

```lean
-- 贝叶斯网络
def bayesian_network (V : set node) (E : set edge) (P : set probability) :=
  (V, E) : DAG ∧
  P(Xᵢ | parents(Xᵢ)) : conditional_probability

-- 联合概率
def joint_probability (X : set variable) :=
  P(X) = ∏ᵢ P(Xᵢ | parents(Xᵢ))
```

#### 算法实现12

```rust
// 贝叶斯网络
struct BayesianNetwork;

impl BayesianNetwork {
    fn joint_probability(&self, variables: &[f64], conditional_probs: &[Vec<f64>], 
                        parent_indices: &[Vec<usize>]) -> f64 {
        let mut joint_prob = 1.0;
        
        for (i, &var) in variables.iter().enumerate() {
            let parents = &parent_indices[i];
            let mut parent_values = Vec::new();
            
            for &parent_idx in parents {
                parent_values.push(variables[parent_idx]);
            }
            
            let conditional_prob = self.get_conditional_probability(i, &parent_values, conditional_probs);
            joint_prob *= conditional_prob;
        }
        
        joint_prob
    }
    
    fn get_conditional_probability(&self, node: usize, parent_values: &[f64], 
                                 conditional_probs: &[Vec<f64>]) -> f64 {
        // 简化版本，实际应用中需要更复杂的条件概率表
        let base_prob = conditional_probs[node][0];
        let mut adjustment = 0.0;
        
        for (i, &parent_val) in parent_values.iter().enumerate() {
            adjustment += parent_val * conditional_probs[node][i + 1];
        }
        
        (base_prob + adjustment).max(0.0).min(1.0)
    }
    
    fn inference(&self, evidence: &[Option<f64>], conditional_probs: &[Vec<f64>], 
                parent_indices: &[Vec<usize>], query_node: usize) -> Vec<f64> {
        let n_states = 10; // 简化版本
        let mut posterior = vec![0.0; n_states];
        
        // 枚举所有可能的变量值
        for state in 0..n_states {
            let mut variables = vec![0.0; evidence.len()];
            
            // 设置证据变量
            for (i, &ev) in evidence.iter().enumerate() {
                if let Some(val) = ev {
                    variables[i] = val;
                } else {
                    variables[i] = state as f64 / n_states as f64;
                }
            }
            
            let joint_prob = self.joint_probability(&variables, conditional_probs, parent_indices);
            posterior[state] = joint_prob;
        }
        
        // 归一化
        let sum: f64 = posterior.iter().sum();
        for p in &mut posterior {
            *p /= sum;
        }
        
        posterior
    }
}
```

## 实际应用

### 经典问题

#### 医学诊断

```rust
// 医学诊断
struct MedicalDiagnosis;

impl MedicalDiagnosis {
    fn disease_probability(&self, symptoms: &[bool], sensitivity: f64, specificity: f64, 
                          prevalence: f64) -> f64 {
        let mut likelihood = 1.0;
        let mut specificity_term = 1.0;
        
        for &symptom in symptoms {
            if symptom {
                likelihood *= sensitivity;
                specificity_term *= (1.0 - specificity);
            } else {
                likelihood *= (1.0 - sensitivity);
                specificity_term *= specificity;
            }
        }
        
        let prior = prevalence;
        let evidence = likelihood * prior + specificity_term * (1.0 - prior);
        
        if evidence > 0.0 {
            likelihood * prior / evidence
        } else {
            0.0
        }
    }
    
    fn sequential_diagnosis(&self, initial_prob: f64, test_results: &[bool], 
                           sensitivities: &[f64], specificities: &[f64]) -> Vec<f64> {
        let mut probabilities = Vec::new();
        let mut current_prob = initial_prob;
        
        for (i, &test_result) in test_results.iter().enumerate() {
            let sensitivity = sensitivities[i];
            let specificity = specificities[i];
            
            if test_result {
                current_prob = (sensitivity * current_prob) / 
                              (sensitivity * current_prob + (1.0 - specificity) * (1.0 - current_prob));
            } else {
                current_prob = ((1.0 - sensitivity) * current_prob) / 
                              ((1.0 - sensitivity) * current_prob + specificity * (1.0 - current_prob));
            }
            
            probabilities.push(current_prob);
        }
        
        probabilities
    }
}
```

#### 垃圾邮件过滤

```rust
// 垃圾邮件过滤
struct SpamFilter;

impl SpamFilter {
    fn naive_bayes_classifier(&self, words: &[String], word_probs_spam: &[f64], 
                              word_probs_ham: &[f64], prior_spam: f64) -> f64 {
        let mut log_likelihood_spam = 0.0;
        let mut log_likelihood_ham = 0.0;
        
        for word in words {
            // 简化版本，实际应用中需要词汇表映射
            let word_index = word.len() % word_probs_spam.len();
            let p_spam = word_probs_spam[word_index];
            let p_ham = word_probs_ham[word_index];
            
            log_likelihood_spam += p_spam.ln();
            log_likelihood_ham += p_ham.ln();
        }
        
        let log_prior_spam = prior_spam.ln();
        let log_prior_ham = (1.0 - prior_spam).ln();
        
        let log_posterior_spam = log_likelihood_spam + log_prior_spam;
        let log_posterior_ham = log_likelihood_ham + log_prior_ham;
        
        let max_log = log_posterior_spam.max(log_posterior_ham);
        let normalized_spam = (log_posterior_spam - max_log).exp();
        let normalized_ham = (log_posterior_ham - max_log).exp();
        
        normalized_spam / (normalized_spam + normalized_ham)
    }
    
    fn update_word_probabilities(&self, words: &[String], is_spam: bool, 
                                word_counts_spam: &mut [usize], word_counts_ham: &mut [usize],
                                total_spam: &mut usize, total_ham: &mut usize) {
        for word in words {
            let word_index = word.len() % word_counts_spam.len();
            
            if is_spam {
                word_counts_spam[word_index] += 1;
                *total_spam += 1;
            } else {
                word_counts_ham[word_index] += 1;
                *total_ham += 1;
            }
        }
    }
    
    fn calculate_word_probabilities(&self, word_counts_spam: &[usize], word_counts_ham: &[usize],
                                   total_spam: usize, total_ham: usize, alpha: f64) -> (Vec<f64>, Vec<f64>) {
        let mut word_probs_spam = Vec::new();
        let mut word_probs_ham = Vec::new();
        
        for i in 0..word_counts_spam.len() {
            let p_spam = (word_counts_spam[i] as f64 + alpha) / (total_spam as f64 + 2.0 * alpha);
            let p_ham = (word_counts_ham[i] as f64 + alpha) / (total_ham as f64 + 2.0 * alpha);
            
            word_probs_spam.push(p_spam);
            word_probs_ham.push(p_ham);
        }
        
        (word_probs_spam, word_probs_ham)
    }
}
```

### 复杂问题

#### 推荐系统

```rust
// 贝叶斯推荐系统
struct BayesianRecommender;

impl BayesianRecommender {
    fn collaborative_filtering(&self, user_ratings: &[Vec<Option<f64>>], 
                              user_id: usize, item_id: usize) -> f64 {
        let n_users = user_ratings.len();
        let n_items = user_ratings[0].len();
        
        // 计算用户相似度
        let mut similarities = Vec::new();
        for other_user in 0..n_users {
            if other_user != user_id {
                let similarity = self.calculate_similarity(user_ratings, user_id, other_user);
                similarities.push((other_user, similarity));
            }
        }
        
        // 排序并选择最相似的用户
        similarities.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap());
        
        let mut weighted_sum = 0.0;
        let mut weight_sum = 0.0;
        
        for (other_user, similarity) in similarities.iter().take(10) {
            if let Some(rating) = user_ratings[*other_user][item_id] {
                weighted_sum += similarity * rating;
                weight_sum += similarity;
            }
        }
        
        if weight_sum > 0.0 {
            weighted_sum / weight_sum
        } else {
            3.0 // 默认评分
        }
    }
    
    fn calculate_similarity(&self, user_ratings: &[Vec<Option<f64>>], user1: usize, user2: usize) -> f64 {
        let mut common_items = 0;
        let mut sum_diff_squared = 0.0;
        
        for item in 0..user_ratings[0].len() {
            if let (Some(rating1), Some(rating2)) = (user_ratings[user1][item], user_ratings[user2][item]) {
                sum_diff_squared += (rating1 - rating2).powi(2);
                common_items += 1;
            }
        }
        
        if common_items > 0 {
            let mse = sum_diff_squared / common_items as f64;
            (-mse).exp() // 使用指数相似度
        } else {
            0.0
        }
    }
    
    fn bayesian_matrix_factorization(&self, user_ratings: &[Vec<Option<f64>>], 
                                    n_factors: usize, n_iterations: usize) -> (Vec<Vec<f64>>, Vec<Vec<f64>>) {
        let n_users = user_ratings.len();
        let n_items = user_ratings[0].len();
        
        // 初始化用户和物品因子矩阵
        let mut user_factors = vec![vec![0.1; n_factors]; n_users];
        let mut item_factors = vec![vec![0.1; n_factors]; n_items];
        
        // 简化的贝叶斯更新
        for _ in 0..n_iterations {
            for user in 0..n_users {
                for item in 0..n_items {
                    if let Some(rating) = user_ratings[user][item] {
                        let prediction = self.predict_rating(&user_factors[user], &item_factors[item]);
                        let error = rating - prediction;
                        
                        // 更新因子
                        for f in 0..n_factors {
                            user_factors[user][f] += 0.01 * error * item_factors[item][f];
                            item_factors[item][f] += 0.01 * error * user_factors[user][f];
                        }
                    }
                }
            }
        }
        
        (user_factors, item_factors)
    }
    
    fn predict_rating(&self, user_factors: &[f64], item_factors: &[f64]) -> f64 {
        user_factors.iter()
            .zip(item_factors.iter())
            .map(|(u, i)| u * i)
            .sum()
    }
}
```

## 教学策略

### 多表征学习

- **符号表征**：数学公式和符号
- **图形表征**：贝叶斯网络图、概率树
- **数值表征**：具体数值和计算
- **物理表征**：实际数据和实验

### 认知负荷管理

- **分步教学**：从简单到复杂
- **渐进复杂**：逐步增加难度
- **多角度验证**：多种方法验证结果

### 错误预防

- **常见错误**：
  - 混淆先验和后验
  - 忽略模型假设
  - 误解贝叶斯因子
- **预防策略**：
  - 强调概念理解
  - 多练习和验证
  - 系统化检查

## 评估标准

### 理解层面

- 能否解释贝叶斯统计概念
- 能否理解不同贝叶斯方法
- 能否识别适用条件

### 应用层面

- 能否正确进行贝叶斯推断
- 能否构建贝叶斯模型
- 能否解决实际问题

### 创新层面

- 能否发现新的应用
- 能否优化贝叶斯方法
- 能否建立新的联系

## 扩展阅读

### 理论扩展

- **变分推断**：变分贝叶斯、期望传播
- **深度贝叶斯**：贝叶斯神经网络
- **非参数贝叶斯**：高斯过程、狄利克雷过程

### 应用扩展

- **机器学习**：贝叶斯优化、主动学习
- **人工智能**：贝叶斯网络、因果推断
- **决策科学**：贝叶斯决策分析

### 历史文献

- 贝叶斯《论机会学说》
- 拉普拉斯《概率论》
- 杰弗里斯《概率论》

## 学习资源

### 推荐教材

- 《贝叶斯统计推断》- 罗伯特
- 《贝叶斯数据分析》- 格尔曼
- 《贝叶斯方法》- 伯杰

### 在线资源

- Coursera贝叶斯统计课程
- edX机器学习课程
- MIT OpenCourseWare统计课程

### 实践工具

- R语言：rstan, rjags
- Python：pymc3, pystan
- JAGS：贝叶斯建模

---

*贝叶斯统计为不确定性量化提供了强大的数学工具，是现代统计学和人工智能的重要基础。*
