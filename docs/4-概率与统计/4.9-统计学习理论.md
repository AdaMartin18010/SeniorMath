# 统计学习理论

## 概述

统计学习理论是机器学习的重要理论基础，研究从数据中学习的数学原理。本模块涵盖VC维、经验风险最小化、结构风险最小化、支持向量机等核心内容，为机器学习算法提供理论基础。

## 历史与发展

### 历史背景

- **早期发展**：20世纪60年代瓦普尼克建立统计学习理论
- **理论完善**：瓦普尼克、切尔沃嫩基斯发展VC理论
- **现代发展**：核方法、深度学习、大数据学习
- **应用扩展**：人工智能、数据科学、模式识别等

### 数学意义

```lean
-- 统计学习理论的形式化定义
def statistical_learning_theory (X : input_space) (Y : output_space) (f : hypothesis) :=
  risk(f) = E[L(f(X), Y)] ∧
  empirical_risk(f) = (1/n)∑ᵢ L(f(Xᵢ), Yᵢ) ∧
  generalization_bound : P(sup_f |risk(f) - empirical_risk(f)| > ε) ≤ δ
```

## VC维理论

### VC维定义

#### 基本概念

```lean
-- VC维定义
def vc_dimension (H : hypothesis_class) (X : input_space) :=
  VC(H) = max{d : ∃ S ⊆ X, |S| = d ∧ H shatters S} ∧
  shatters(H, S) = ∀ labeling : S → {±1}, ∃ h ∈ H : h|S = labeling

-- 增长函数
def growth_function (H : hypothesis_class) (n : sample_size) :=
  Π_H(n) = max{|H|S| : S ⊆ X, |S| = n}
```

#### 算法实现

```rust
// VC维计算
struct VCDimension;

impl VCDimension {
    fn calculate_vc_dimension(&self, hypothesis_class: &[Vec<bool>], input_space: &[Vec<f64>]) -> usize {
        let mut max_vc = 0;
        
        for subset_size in 1..=input_space.len() {
            let subsets = self.generate_subsets(input_space, subset_size);
            
            for subset in subsets {
                if self.can_shatter(hypothesis_class, &subset) {
                    max_vc = max_vc.max(subset_size);
                }
            }
        }
        
        max_vc
    }
    
    fn generate_subsets(&self, elements: &[Vec<f64>], size: usize) -> Vec<Vec<Vec<f64>>> {
        let mut subsets = Vec::new();
        let n = elements.len();
        
        if size > n {
            return subsets;
        }
        
        let mut indices: Vec<usize> = (0..size).collect();
        
        loop {
            let mut subset = Vec::new();
            for &index in &indices {
                subset.push(elements[index].clone());
            }
            subsets.push(subset);
            
            // 生成下一个组合
            let mut i = size - 1;
            while i > 0 && indices[i] == n - size + i {
                i -= 1;
            }
            
            if i == 0 && indices[i] == n - size {
                break;
            }
            
            indices[i] += 1;
            for j in (i + 1)..size {
                indices[j] = indices[j - 1] + 1;
            }
        }
        
        subsets
    }
    
    fn can_shatter(&self, hypothesis_class: &[Vec<bool>], subset: &[Vec<f64>]) -> bool {
        let n = subset.len();
        let total_labelings = 1 << n;
        
        let mut achievable_labelings = 0;
        
        for hypothesis in hypothesis_class {
            let mut labeling = Vec::new();
            for point in subset {
                let prediction = self.apply_hypothesis(hypothesis, point);
                labeling.push(prediction);
            }
            achievable_labelings += 1;
        }
        
        achievable_labelings >= total_labelings
    }
    
    fn apply_hypothesis(&self, hypothesis: &[bool], point: &[f64]) -> bool {
        // 简化版本，实际应用中需要更复杂的假设函数
        let sum: f64 = point.iter().zip(hypothesis.iter())
            .map(|(x, &h)| if h { *x } else { -*x })
            .sum();
        sum > 0.0
    }
    
    fn sauer_shelah_lemma(&self, vc_dim: usize, sample_size: usize) -> usize {
        let mut growth = 0;
        for i in 0..=vc_dim {
            growth += self.binomial_coefficient(sample_size, i);
        }
        growth
    }
    
    fn binomial_coefficient(&self, n: usize, k: usize) -> usize {
        if k > n {
            return 0;
        }
        if k == 0 || k == n {
            return 1;
        }
        
        let mut result = 1;
        for i in 0..k {
            result = result * (n - i) / (i + 1);
        }
        result
    }
}
```

### 泛化界

#### 基本定理

```lean
-- 泛化界
theorem generalization_bound (H : hypothesis_class) (n : sample_size) (δ : confidence) :=
  P(sup_{h∈H} |R(h) - R̂(h)| > ε) ≤ δ ∧
  ε = √((log|H| + log(1/δ))/n)

-- VC泛化界
theorem vc_generalization_bound (H : hypothesis_class) (d : vc_dimension) (n : sample_size) :=
  P(sup_{h∈H} |R(h) - R̂(h)| > ε) ≤ 4Π_H(2n)exp(-nε²/8) ∧
  ε = √((d log(n/d) + log(1/δ))/n)
```

#### 算法实现1

```rust
// 泛化界计算
struct GeneralizationBound;

impl GeneralizationBound {
    fn calculate_bound(&self, vc_dim: usize, sample_size: usize, confidence: f64) -> f64 {
        let delta = 1.0 - confidence;
        let epsilon = ((vc_dim as f64 * (sample_size as f64 / vc_dim as f64).ln() + 
                       (1.0 / delta).ln()) / sample_size as f64).sqrt();
        epsilon
    }
    
    fn calculate_rademacher_bound(&self, hypothesis_class_size: usize, sample_size: usize, confidence: f64) -> f64 {
        let delta = 1.0 - confidence;
        let epsilon = ((hypothesis_class_size.ln() + (1.0 / delta).ln()) / sample_size as f64).sqrt();
        epsilon
    }
    
    fn calculate_pac_bound(&self, error_rate: f64, sample_size: usize, confidence: f64) -> f64 {
        let delta = 1.0 - confidence;
        let epsilon = ((1.0 / (2.0 * sample_size as f64)) * (2.0 / delta).ln()).sqrt();
        error_rate + epsilon
    }
    
    fn calculate_union_bound(&self, events: &[f64]) -> f64 {
        events.iter().sum::<f64>().min(1.0)
    }
    
    fn calculate_hoeffding_bound(&self, sample_size: usize, confidence: f64) -> f64 {
        let delta = 1.0 - confidence;
        ((1.0 / (2.0 * sample_size as f64)) * (2.0 / delta).ln()).sqrt()
    }
}
```

## 经验风险最小化

### 基本概念1

```lean
-- 经验风险最小化
def empirical_risk_minimization (H : hypothesis_class) (S : sample) :=
  h_ERM = argmin_{h∈H} R̂(h) ∧
  R̂(h) = (1/n)∑ᵢ L(h(Xᵢ), Yᵢ)

-- 一致性
def consistency (H : hypothesis_class) (L : loss_function) :=
  ∀ ε > 0, lim_{n→∞} P(R(h_ERM) - R(h*) > ε) = 0
```

### 算法实现2

```rust
// 经验风险最小化
struct EmpiricalRiskMinimization;

impl EmpiricalRiskMinimization {
    fn minimize(&self, hypothesis_class: &[Vec<f64>], training_data: &[(Vec<f64>, f64)], 
                loss_function: fn(&[f64], &Vec<f64>, f64) -> f64) -> (Vec<f64>, f64) {
        let mut best_hypothesis = hypothesis_class[0].clone();
        let mut best_risk = f64::INFINITY;
        
        for hypothesis in hypothesis_class {
            let empirical_risk = self.calculate_empirical_risk(hypothesis, training_data, loss_function);
            
            if empirical_risk < best_risk {
                best_risk = empirical_risk;
                best_hypothesis = hypothesis.clone();
            }
        }
        
        (best_hypothesis, best_risk)
    }
    
    fn calculate_empirical_risk(&self, hypothesis: &[f64], training_data: &[(Vec<f64>, f64)], 
                               loss_function: fn(&[f64], &Vec<f64>, f64) -> f64) -> f64 {
        let n = training_data.len();
        let mut total_loss = 0.0;
        
        for (input, target) in training_data {
            let prediction = self.predict(hypothesis, input);
            total_loss += loss_function(hypothesis, input, *target);
        }
        
        total_loss / n as f64
    }
    
    fn predict(&self, hypothesis: &[f64], input: &[f64]) -> f64 {
        hypothesis.iter().zip(input.iter())
            .map(|(h, x)| h * x)
            .sum()
    }
    
    fn zero_one_loss(&self, hypothesis: &[f64], input: &[Vec<f64>], target: f64) -> f64 {
        let prediction = self.predict(hypothesis, input);
        if (prediction > 0.0 && target > 0.0) || (prediction <= 0.0 && target <= 0.0) {
            0.0
        } else {
            1.0
        }
    }
    
    fn squared_loss(&self, hypothesis: &[f64], input: &[Vec<f64>], target: f64) -> f64 {
        let prediction = self.predict(hypothesis, input);
        (prediction - target).powi(2)
    }
    
    fn hinge_loss(&self, hypothesis: &[f64], input: &[Vec<f64>], target: f64) -> f64 {
        let prediction = self.predict(hypothesis, input);
        (1.0 - target * prediction).max(0.0)
    }
}
```

## 结构风险最小化

### 基本概念2

```lean
-- 结构风险最小化
def structural_risk_minimization (H : hypothesis_class) (S : sample) (λ : regularization) :=
  h_SRM = argmin_{h∈H} R̂(h) + λ||h||² ∧
  ||h||² = ∑ᵢ hᵢ²

-- 正则化
def regularization (H : hypothesis_class) (λ : parameter) :=
  R_reg(h) = R̂(h) + λΩ(h) ∧
  Ω(h) : complexity_penalty
```

### 算法实现3

```rust
// 结构风险最小化
struct StructuralRiskMinimization;

impl StructuralRiskMinimization {
    fn minimize(&self, hypothesis_class: &[Vec<f64>], training_data: &[(Vec<f64>, f64)], 
                lambda: f64, loss_function: fn(&[f64], &Vec<f64>, f64) -> f64) -> (Vec<f64>, f64) {
        let mut best_hypothesis = hypothesis_class[0].clone();
        let mut best_risk = f64::INFINITY;
        
        for hypothesis in hypothesis_class {
            let empirical_risk = self.calculate_empirical_risk(hypothesis, training_data, loss_function);
            let regularization_term = lambda * self.calculate_norm(hypothesis);
            let structural_risk = empirical_risk + regularization_term;
            
            if structural_risk < best_risk {
                best_risk = structural_risk;
                best_hypothesis = hypothesis.clone();
            }
        }
        
        (best_hypothesis, best_risk)
    }
    
    fn calculate_empirical_risk(&self, hypothesis: &[f64], training_data: &[(Vec<f64>, f64)], 
                               loss_function: fn(&[f64], &Vec<f64>, f64) -> f64) -> f64 {
        let n = training_data.len();
        let mut total_loss = 0.0;
        
        for (input, target) in training_data {
            total_loss += loss_function(hypothesis, input, *target);
        }
        
        total_loss / n as f64
    }
    
    fn calculate_norm(&self, hypothesis: &[f64]) -> f64 {
        hypothesis.iter().map(|h| h * h).sum()
    }
    
    fn l1_regularization(&self, hypothesis: &[f64]) -> f64 {
        hypothesis.iter().map(|h| h.abs()).sum()
    }
    
    fn l2_regularization(&self, hypothesis: &[f64]) -> f64 {
        self.calculate_norm(hypothesis)
    }
    
    fn elastic_net_regularization(&self, hypothesis: &[f64], alpha: f64, lambda: f64) -> f64 {
        let l1_term = alpha * self.l1_regularization(hypothesis);
        let l2_term = (1.0 - alpha) * self.l2_regularization(hypothesis);
        lambda * (l1_term + l2_term)
    }
}
```

## 支持向量机

### 线性SVM

#### 基本概念3

```lean
-- 线性支持向量机
def linear_svm (X : input_space) (Y : output_space) (w : weight_vector) (b : bias) :=
  f(x) = sign(w·x + b) ∧
  margin = 2/||w|| ∧
  optimization : min ||w||²/2 s.t. yᵢ(w·xᵢ + b) ≥ 1
```

#### 算法实现4

```rust
// 线性支持向量机
struct LinearSVM;

impl LinearSVM {
    fn train(&self, training_data: &[(Vec<f64>, f64)], c: f64) -> (Vec<f64>, f64) {
        let n = training_data.len();
        let d = training_data[0].0.len();
        
        // 构建二次规划问题
        let mut q_matrix = vec![vec![0.0; n]; n];
        let mut p_vector = vec![-1.0; n];
        
        for i in 0..n {
            for j in 0..n {
                let (x_i, y_i) = &training_data[i];
                let (x_j, y_j) = &training_data[j];
                q_matrix[i][j] = y_i * y_j * self.dot_product(x_i, x_j);
            }
        }
        
        // 求解对偶问题（简化版本）
        let alpha = self.solve_quadratic_programming(&q_matrix, &p_vector, c);
        
        // 计算权重向量
        let mut w = vec![0.0; d];
        for i in 0..n {
            let (x_i, y_i) = &training_data[i];
            for j in 0..d {
                w[j] += alpha[i] * y_i * x_i[j];
            }
        }
        
        // 计算偏置
        let b = self.calculate_bias(&training_data, &w, &alpha);
        
        (w, b)
    }
    
    fn dot_product(&self, x1: &[f64], x2: &[f64]) -> f64 {
        x1.iter().zip(x2.iter())
            .map(|(a, b)| a * b)
            .sum()
    }
    
    fn solve_quadratic_programming(&self, q_matrix: &[Vec<f64>], p_vector: &[f64], c: f64) -> Vec<f64> {
        // 简化版本，实际应用中需要使用专门的QP求解器
        let n = p_vector.len();
        let mut alpha = vec![c / n as f64; n];
        
        // 简单的梯度下降
        for _ in 0..100 {
            for i in 0..n {
                let mut gradient = p_vector[i];
                for j in 0..n {
                    gradient += q_matrix[i][j] * alpha[j];
                }
                alpha[i] = (alpha[i] - 0.01 * gradient).max(0.0).min(c);
            }
        }
        
        alpha
    }
    
    fn calculate_bias(&self, training_data: &[(Vec<f64>, f64)], w: &[f64], alpha: &[f64]) -> f64 {
        let mut b_sum = 0.0;
        let mut count = 0;
        
        for (i, (x, y)) in training_data.iter().enumerate() {
            if alpha[i] > 0.001 && alpha[i] < 0.999 {
                let prediction = self.predict(w, 0.0, x);
                b_sum += y - prediction;
                count += 1;
            }
        }
        
        if count > 0 {
            b_sum / count as f64
        } else {
            0.0
        }
    }
    
    fn predict(&self, w: &[f64], b: f64, x: &[f64]) -> f64 {
        let score = self.dot_product(w, x) + b;
        if score > 0.0 { 1.0 } else { -1.0 }
    }
}
```

### 核SVM

```rust
// 核支持向量机
struct KernelSVM;

impl KernelSVM {
    fn train(&self, training_data: &[(Vec<f64>, f64)], c: f64, kernel_type: &str) -> (Vec<f64>, Vec<f64>) {
        let n = training_data.len();
        
        // 构建核矩阵
        let mut k_matrix = vec![vec![0.0; n]; n];
        for i in 0..n {
            for j in 0..n {
                let (x_i, _) = &training_data[i];
                let (x_j, _) = &training_data[j];
                k_matrix[i][j] = self.kernel_function(x_i, x_j, kernel_type);
            }
        }
        
        // 求解对偶问题
        let alpha = self.solve_kernel_qp(&k_matrix, training_data, c);
        
        (alpha, vec![0.0; n]) // 简化版本，实际需要支持向量
    }
    
    fn kernel_function(&self, x1: &[f64], x2: &[f64], kernel_type: &str) -> f64 {
        match kernel_type {
            "linear" => self.linear_kernel(x1, x2),
            "polynomial" => self.polynomial_kernel(x1, x2, 2.0, 1.0, 1.0),
            "rbf" => self.rbf_kernel(x1, x2, 1.0),
            "sigmoid" => self.sigmoid_kernel(x1, x2, 1.0, 0.0),
            _ => self.linear_kernel(x1, x2),
        }
    }
    
    fn linear_kernel(&self, x1: &[f64], x2: &[f64]) -> f64 {
        x1.iter().zip(x2.iter())
            .map(|(a, b)| a * b)
            .sum()
    }
    
    fn polynomial_kernel(&self, x1: &[f64], x2: &[f64], degree: f64, gamma: f64, coef0: f64) -> f64 {
        let dot_product = self.linear_kernel(x1, x2);
        (gamma * dot_product + coef0).powf(degree)
    }
    
    fn rbf_kernel(&self, x1: &[f64], x2: &[f64], gamma: f64) -> f64 {
        let distance_squared: f64 = x1.iter().zip(x2.iter())
            .map(|(a, b)| (a - b).powi(2))
            .sum();
        (-gamma * distance_squared).exp()
    }
    
    fn sigmoid_kernel(&self, x1: &[f64], x2: &[f64], gamma: f64, coef0: f64) -> f64 {
        let dot_product = self.linear_kernel(x1, x2);
        (gamma * dot_product + coef0).tanh()
    }
    
    fn solve_kernel_qp(&self, k_matrix: &[Vec<f64>], training_data: &[(Vec<f64>, f64)], c: f64) -> Vec<f64> {
        let n = training_data.len();
        let mut alpha = vec![c / n as f64; n];
        
        // 简化的SMO算法
        for _ in 0..100 {
            for i in 0..n {
                let mut gradient = -1.0;
                for j in 0..n {
                    let (_, y_i) = training_data[i];
                    let (_, y_j) = training_data[j];
                    gradient += y_i * y_j * k_matrix[i][j] * alpha[j];
                }
                alpha[i] = (alpha[i] - 0.01 * gradient).max(0.0).min(c);
            }
        }
        
        alpha
    }
}
```

## 实际应用

### 经典问题

#### 分类问题

```rust
// 分类问题
struct ClassificationProblem;

impl ClassificationProblem {
    fn binary_classification(&self, training_data: &[(Vec<f64>, f64)], test_data: &[(Vec<f64>, f64)]) -> ClassificationResults {
        let svm = LinearSVM;
        let (w, b) = svm.train(training_data, 1.0);
        
        let mut predictions = Vec::new();
        let mut correct = 0;
        
        for (x, y_true) in test_data {
            let y_pred = svm.predict(&w, b, x);
            predictions.push(y_pred);
            if y_pred == *y_true {
                correct += 1;
            }
        }
        
        let accuracy = correct as f64 / test_data.len() as f64;
        
        ClassificationResults {
            accuracy,
            predictions,
            support_vectors: vec![w, vec![b]],
        }
    }
    
    fn multi_class_classification(&self, training_data: &[(Vec<f64>, usize)], test_data: &[(Vec<f64>, usize)]) -> MultiClassResults {
        let n_classes = training_data.iter().map(|(_, y)| *y).max().unwrap() + 1;
        let mut classifiers = Vec::new();
        
        // 一对多分类
        for class in 0..n_classes {
            let binary_data: Vec<(Vec<f64>, f64)> = training_data.iter()
                .map(|(x, y)| (x.clone(), if *y == class { 1.0 } else { -1.0 }))
                .collect();
            
            let svm = LinearSVM;
            let (w, b) = svm.train(&binary_data, 1.0);
            classifiers.push((w, b));
        }
        
        let mut predictions = Vec::new();
        let mut correct = 0;
        
        for (x, y_true) in test_data {
            let mut scores = Vec::new();
            for (w, b) in &classifiers {
                let score = svm.predict(w, *b, x);
                scores.push(score);
            }
            
            let predicted_class = scores.iter().enumerate()
                .max_by(|(_, a), (_, b)| a.partial_cmp(b).unwrap())
                .unwrap().0;
            
            predictions.push(predicted_class);
            if predicted_class == *y_true {
                correct += 1;
            }
        }
        
        let accuracy = correct as f64 / test_data.len() as f64;
        
        MultiClassResults {
            accuracy,
            predictions,
            n_classes,
        }
    }
}

struct ClassificationResults {
    accuracy: f64,
    predictions: Vec<f64>,
    support_vectors: Vec<Vec<f64>>,
}

struct MultiClassResults {
    accuracy: f64,
    predictions: Vec<usize>,
    n_classes: usize,
}
```

#### 回归问题

```rust
// 回归问题
struct RegressionProblem;

impl RegressionProblem {
    fn support_vector_regression(&self, training_data: &[(Vec<f64>, f64)], epsilon: f64, c: f64) -> RegressionResults {
        let n = training_data.len();
        let d = training_data[0].0.len();
        
        // 构建回归问题的对偶形式
        let mut q_matrix = vec![vec![0.0; 2*n]; 2*n];
        let mut p_vector = vec![0.0; 2*n];
        
        for i in 0..n {
            for j in 0..n {
                let (x_i, _) = &training_data[i];
                let (x_j, _) = &training_data[j];
                let kernel_value = self.linear_kernel(x_i, x_j);
                
                q_matrix[i][j] = kernel_value;
                q_matrix[i][j+n] = -kernel_value;
                q_matrix[i+n][j] = -kernel_value;
                q_matrix[i+n][j+n] = kernel_value;
            }
            
            let (_, y_i) = training_data[i];
            p_vector[i] = epsilon - y_i;
            p_vector[i+n] = epsilon + y_i;
        }
        
        // 求解对偶问题
        let alpha = self.solve_regression_qp(&q_matrix, &p_vector, c);
        
        // 计算权重和偏置
        let mut w = vec![0.0; d];
        for i in 0..n {
            let (x_i, _) = &training_data[i];
            let alpha_diff = alpha[i] - alpha[i+n];
            for j in 0..d {
                w[j] += alpha_diff * x_i[j];
            }
        }
        
        let b = self.calculate_regression_bias(&training_data, &w, &alpha, epsilon);
        
        RegressionResults {
            weights: w,
            bias: b,
            support_vectors: alpha,
        }
    }
    
    fn linear_kernel(&self, x1: &[f64], x2: &[f64]) -> f64 {
        x1.iter().zip(x2.iter())
            .map(|(a, b)| a * b)
            .sum()
    }
    
    fn solve_regression_qp(&self, q_matrix: &[Vec<f64>], p_vector: &[f64], c: f64) -> Vec<f64> {
        let n = p_vector.len();
        let mut alpha = vec![c / n as f64; n];
        
        // 简化的梯度下降
        for _ in 0..100 {
            for i in 0..n {
                let mut gradient = p_vector[i];
                for j in 0..n {
                    gradient += q_matrix[i][j] * alpha[j];
                }
                alpha[i] = (alpha[i] - 0.01 * gradient).max(0.0).min(c);
            }
        }
        
        alpha
    }
    
    fn calculate_regression_bias(&self, training_data: &[(Vec<f64>, f64)], w: &[f64], alpha: &[f64], epsilon: f64) -> f64 {
        let mut b_sum = 0.0;
        let mut count = 0;
        
        for (i, (x, y)) in training_data.iter().enumerate() {
            let alpha_diff = alpha[i] - alpha[i + training_data.len()];
            if alpha_diff.abs() > 0.001 {
                let prediction = self.predict(w, 0.0, x);
                b_sum += y - prediction;
                count += 1;
            }
        }
        
        if count > 0 {
            b_sum / count as f64
        } else {
            0.0
        }
    }
    
    fn predict(&self, w: &[f64], b: f64, x: &[f64]) -> f64 {
        w.iter().zip(x.iter())
            .map(|(wi, xi)| wi * xi)
            .sum::<f64>() + b
    }
}

struct RegressionResults {
    weights: Vec<f64>,
    bias: f64,
    support_vectors: Vec<f64>,
}
```

### 复杂问题

#### 特征选择

```rust
// 特征选择
struct FeatureSelection;

impl FeatureSelection {
    fn recursive_feature_elimination(&self, training_data: &[(Vec<f64>, f64)], n_features: usize) -> Vec<usize> {
        let mut selected_features: Vec<usize> = (0..training_data[0].0.len()).collect();
        
        while selected_features.len() > n_features {
            let mut worst_feature = 0;
            let mut worst_score = f64::INFINITY;
            
            for &feature in &selected_features {
                let mut temp_features = selected_features.clone();
                temp_features.retain(|&f| f != feature);
                
                let score = self.evaluate_feature_subset(training_data, &temp_features);
                if score < worst_score {
                    worst_score = score;
                    worst_feature = feature;
                }
            }
            
            selected_features.retain(|&f| f != worst_feature);
        }
        
        selected_features
    }
    
    fn evaluate_feature_subset(&self, training_data: &[(Vec<f64>, f64)], features: &[usize]) -> f64 {
        let reduced_data: Vec<(Vec<f64>, f64)> = training_data.iter()
            .map(|(x, y)| {
                let reduced_x: Vec<f64> = features.iter().map(|&i| x[i]).collect();
                (reduced_x, *y)
            })
            .collect();
        
        let svm = LinearSVM;
        let (w, b) = svm.train(&reduced_data, 1.0);
        
        // 使用交叉验证评估
        self.cross_validation_score(&reduced_data, 5)
    }
    
    fn cross_validation_score(&self, data: &[(Vec<f64>, f64)], folds: usize) -> f64 {
        let n = data.len();
        let fold_size = n / folds;
        let mut total_score = 0.0;
        
        for fold in 0..folds {
            let start = fold * fold_size;
            let end = if fold == folds - 1 { n } else { (fold + 1) * fold_size };
            
            let mut train_data = Vec::new();
            let mut test_data = Vec::new();
            
            for i in 0..n {
                if i >= start && i < end {
                    test_data.push(data[i].clone());
                } else {
                    train_data.push(data[i].clone());
                }
            }
            
            let svm = LinearSVM;
            let (w, b) = svm.train(&train_data, 1.0);
            
            let mut correct = 0;
            for (x, y_true) in &test_data {
                let y_pred = svm.predict(&w, b, x);
                if y_pred == *y_true {
                    correct += 1;
                }
            }
            
            total_score += correct as f64 / test_data.len() as f64;
        }
        
        total_score / folds as f64
    }
}
```

## 教学策略

### 多表征学习

- **符号表征**：数学公式和符号
- **图形表征**：决策边界图、核函数图
- **数值表征**：具体数值和计算
- **物理表征**：实际数据和实验

### 认知负荷管理

- **分步教学**：从简单到复杂
- **渐进复杂**：逐步增加难度
- **多角度验证**：多种方法验证结果

### 错误预防

- **常见错误**：
  - 混淆经验风险和真实风险
  - 误解VC维概念
  - 忽略正则化作用
- **预防策略**：
  - 强调概念理解
  - 多练习和验证
  - 系统化检查

## 评估标准

### 理解层面

- 能否解释统计学习理论概念
- 能否理解不同学习方法
- 能否识别适用条件

### 应用层面

- 能否正确应用学习算法
- 能否分析学习性能
- 能否解决实际问题

### 创新层面

- 能否发现新的应用
- 能否优化学习方法
- 能否建立新的联系

## 扩展阅读

### 理论扩展

- **深度学习理论**：深度网络泛化
- **在线学习**：在线算法理论
- **强化学习**：强化学习理论

### 应用扩展

- **计算机视觉**：图像分类、目标检测
- **自然语言处理**：文本分类、机器翻译
- **推荐系统**：协同过滤、内容推荐

### 历史文献

- 瓦普尼克《统计学习理论》
- 瓦普尼克《支持向量机》
- 切尔沃嫩基斯《学习理论》

## 学习资源

### 推荐教材

- 《统计学习理论》- 瓦普尼克
- 《机器学习》- 米切尔
- 《模式识别》- 毕晓普

### 在线资源

- Coursera机器学习课程
- edX统计学习课程
- MIT OpenCourseWare机器学习课程

### 实践工具

- R语言：e1071, kernlab
- Python：scikit-learn, libsvm
- MATLAB：Statistics and Machine Learning Toolbox

---

*统计学习理论为机器学习算法提供了强大的数学工具，是现代人工智能和数据科学的重要基础。*
